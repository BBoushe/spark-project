{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T13:53:14.210009Z",
     "start_time": "2025-02-13T13:53:06.819107Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:53:14.229071Z",
     "start_time": "2025-02-13T13:53:14.218472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "data_path = \"/Users/alek/Downloads/H3/MMDS_H3/data\"\n",
    "dataset_path = os.path.join(data_path, \"dataset/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "trained_model_path = os.path.join(data_path, \"trained_models\")\n",
    "\n",
    "offline_path = os.path.join(trained_model_path, \"offline.csv\")\n",
    "online_path = os.path.join(trained_model_path, \"online.csv\")\n"
   ],
   "id": "639c0d09f9bffca3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create 80/20 random split",
   "id": "eb5f20681c03d4d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:50:58.872182Z",
     "start_time": "2025-02-13T12:50:56.441770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Diabetes_binary is the label column\n",
    "y = df[\"Diabetes_binary\"]  \n",
    "X = df.drop(\"Diabetes_binary\", axis=1)\n",
    "\n",
    "X_offline, X_online, y_offline, y_online = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X_offline + y_offline and X_online + y_online\n",
    "offline = pd.concat([X_offline, y_offline], axis=1)\n",
    "online = pd.concat([X_online, y_online], axis=1)\n",
    "\n",
    "offline.to_csv(offline_path, index=False)\n",
    "online.to_csv(online_path, index=False)"
   ],
   "id": "a62fc7717dd86f17",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Offline Spark functionality",
   "id": "de8e5c16cad24e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:06:42.435556Z",
     "start_time": "2025-02-13T14:06:40.747838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.appName(\"OfflineModelTraining\").getOrCreate()\n",
    "\n",
    "df_offline = spark.read.csv(offline_path, header=True, inferSchema=True) # we're inferring the schema, but we can create one as well\n",
    "\n",
    "label_column = \"Diabetes_binary\"\n",
    "feature_columns = [c for c in df_offline.columns if c != label_column]\n",
    "df_offline = df_offline.withColumnRenamed(label_column, \"label\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\") # assembles feature vector\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withStd = True, withMean = False)\n",
    "\n",
    "# Classifiers\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol=\"label\")\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol=\"label\")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "paramGridLR = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridRF = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [10, 20, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt', 'log2'])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridGBT = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10, 20])\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=pipeline_lr,\n",
    "    estimatorParamMaps=paramGridLR,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGridRF,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGridGBT,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ],
   "id": "1e38d1575db96241",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:07:59.446842Z",
     "start_time": "2025-02-13T14:06:45.306899Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_lr = cv_lr.fit(df_offline)   # fits pipeline_lr with paramGridLR",
   "id": "5aa29e548470c9e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 15:06:45 WARN CacheManager: Asked to cache already cached data.\n",
      "25/02/13 15:06:45 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:11:44.573397Z",
     "start_time": "2025-02-13T14:07:59.449587Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_rf = cv_rf.fit(df_offline)   # fits pipeline_rf with paramGridRF",
   "id": "bd5357b55e5780d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 15:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1340.8 KiB\n",
      "25/02/13 15:08:10 WARN DAGScheduler: Broadcasting large task binary with size 1340.8 KiB\n",
      "25/02/13 15:08:12 WARN DAGScheduler: Broadcasting large task binary with size 1340.8 KiB\n",
      "25/02/13 15:08:18 WARN DAGScheduler: Broadcasting large task binary with size 1440.3 KiB\n",
      "25/02/13 15:08:19 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:08:20 WARN DAGScheduler: Broadcasting large task binary with size 1281.1 KiB\n",
      "25/02/13 15:08:22 WARN DAGScheduler: Broadcasting large task binary with size 1440.3 KiB\n",
      "25/02/13 15:08:22 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:08:23 WARN DAGScheduler: Broadcasting large task binary with size 1281.1 KiB\n",
      "25/02/13 15:08:26 WARN DAGScheduler: Broadcasting large task binary with size 1440.3 KiB\n",
      "25/02/13 15:08:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:08:27 WARN DAGScheduler: Broadcasting large task binary with size 1281.1 KiB\n",
      "25/02/13 15:08:37 WARN DAGScheduler: Broadcasting large task binary with size 1853.6 KiB\n",
      "25/02/13 15:08:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:08:41 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:08:46 WARN DAGScheduler: Broadcasting large task binary with size 1489.7 KiB\n",
      "25/02/13 15:08:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:08:51 WARN DAGScheduler: Broadcasting large task binary with size 1853.6 KiB\n",
      "25/02/13 15:08:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:08:54 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:09:00 WARN DAGScheduler: Broadcasting large task binary with size 1489.7 KiB\n",
      "25/02/13 15:09:00 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:09:04 WARN DAGScheduler: Broadcasting large task binary with size 1853.6 KiB\n",
      "25/02/13 15:09:06 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:09:08 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:09:14 WARN DAGScheduler: Broadcasting large task binary with size 1489.7 KiB\n",
      "25/02/13 15:09:15 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:09:21 WARN DAGScheduler: Broadcasting large task binary with size 1314.1 KiB\n",
      "25/02/13 15:09:23 WARN DAGScheduler: Broadcasting large task binary with size 1314.1 KiB\n",
      "25/02/13 15:09:25 WARN DAGScheduler: Broadcasting large task binary with size 1314.1 KiB\n",
      "25/02/13 15:09:31 WARN DAGScheduler: Broadcasting large task binary with size 1430.8 KiB\n",
      "25/02/13 15:09:32 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:09:32 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/02/13 15:09:35 WARN DAGScheduler: Broadcasting large task binary with size 1430.8 KiB\n",
      "25/02/13 15:09:35 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:09:36 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/02/13 15:09:39 WARN DAGScheduler: Broadcasting large task binary with size 1430.8 KiB\n",
      "25/02/13 15:09:39 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:09:40 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/02/13 15:09:51 WARN DAGScheduler: Broadcasting large task binary with size 1862.8 KiB\n",
      "25/02/13 15:09:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:09:54 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:10:12 WARN DAGScheduler: Broadcasting large task binary with size 1503.0 KiB\n",
      "25/02/13 15:10:13 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:11:18 WARN DAGScheduler: Broadcasting large task binary with size 1862.8 KiB\n",
      "25/02/13 15:11:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:11:21 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:11:28 WARN DAGScheduler: Broadcasting large task binary with size 1503.0 KiB\n",
      "25/02/13 15:11:29 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:11:33 WARN DAGScheduler: Broadcasting large task binary with size 1862.8 KiB\n",
      "25/02/13 15:11:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:11:36 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:500)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager.org$apache$spark$storage$BlockManager$RemoteBlockDownloadFileManager$$keepCleaning(BlockManager.scala:2228)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager$$anon$2.run(BlockManager.scala:2194)\n",
      "25/02/13 15:11:43 ERROR Executor: Exception in task 2.0 in stage 13064.0 (TID 58434)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "25/02/13 15:11:43 ERROR Executor: Exception in task 3.0 in stage 13064.0 (TID 58435)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Integer.valueOf(Integer.java:1019)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:561)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002402568.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002402190.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "25/02/13 15:11:43 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#107,Executor task launch worker for task 2.0 in stage 13064.0 (TID 58434),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "25/02/13 15:11:43 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#335,Executor task launch worker for task 3.0 in stage 13064.0 (TID 58435),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Integer.valueOf(Integer.java:1019)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:561)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002402568.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002402190.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "25/02/13 15:11:43 WARN TaskSetManager: Lost task 2.0 in stage 13064.0 (TID 58434) (192.168.180.135 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "25/02/13 15:11:43 ERROR TaskSetManager: Task 2 in stage 13064.0 failed 1 times; aborting job\n",
      "25/02/13 15:11:43 WARN TaskSetManager: Lost task 0.0 in stage 13064.0 (TID 58432) (192.168.180.135 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 13064.0 failed 1 times, most recent failure: Lost task 2.0 in stage 13064.0 (TID 58434) (192.168.180.135 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/02/13 15:11:43 WARN TaskSetManager: Lost task 1.0 in stage 13064.0 (TID 58433) (192.168.180.135 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 13064.0 failed 1 times, most recent failure: Lost task 2.0 in stage 13064.0 (TID 58434) (192.168.180.135 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/02/13 15:11:43 WARN TaskSetManager: Lost task 4.0 in stage 13064.0 (TID 58436) (192.168.180.135 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 13064.0 failed 1 times, most recent failure: Lost task 2.0 in stage 13064.0 (TID 58434) (192.168.180.135 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/02/13 15:11:43 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 13064.0 failed 1 times, most recent failure: Lost task 2.0 in stage 13064.0 (TID 58434) (192.168.180.135 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f8002400da0.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00003f80023e79c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00003f8001e55f98.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00003f8001dcb3e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.runWith(Thread.java:1583)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nz/p6_gy44n7wjfcvscwc4r3gk80000gn/T/ipykernel_16748/1764590812.py\", line 1, in <module>\n",
      "    cvModel_rf = cv_rf.fit(df_offline)   # fits pipeline_rf with paramGridRF\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 870, in next\n",
      "    raise value\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cvModel_rf \u001B[38;5;241m=\u001B[39m \u001B[43mcv_rf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_offline\u001B[49m\u001B[43m)\u001B[49m   \u001B[38;5;66;03m# fits pipeline_rf with paramGridRF\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[1;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[1;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:870\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[0;32m--> 870\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:125\u001B[0m, in \u001B[0;36mworker\u001B[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 125\u001B[0m     result \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit.<locals>.<lambda>\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[1;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, tasks):\n\u001B[1;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/util.py:342\u001B[0m, in \u001B[0;36minheritable_thread_target.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    341\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39msetLocalProperties(properties)\n\u001B[0;32m--> 342\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/tuning.py:113\u001B[0m, in \u001B[0;36m_parallelFitTasks.<locals>.singleTask\u001B[0;34m()\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msingleTask\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, Transformer]:\n\u001B[0;32m--> 113\u001B[0m     index, model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodelIter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001B[39;00m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;66;03m#  all nested stages and evaluators\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py:98\u001B[0m, in \u001B[0;36m_FitMultipleIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfitSingleModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py:156\u001B[0m, in \u001B[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001B[0;34m(index)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfitSingleModel\u001B[39m(index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m M:\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparamMaps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m params:\n\u001B[0;32m--> 203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31m<class 'str'>\u001B[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2152\u001B[0m         traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n\u001B[1;32m   2153\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2156\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n\u001B[1;32m   2157\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n\u001B[1;32m   2158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001B[0m, in \u001B[0;36mZMQInteractiveShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n\u001B[1;32m    553\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    554\u001B[0m sys\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    556\u001B[0m exc_content \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraceback\u001B[39m\u001B[38;5;124m\"\u001B[39m: stb,\n\u001B[1;32m    558\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mename\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(etype\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m),\n\u001B[0;32m--> 559\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevalue\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    560\u001B[0m }\n\u001B[1;32m    562\u001B[0m dh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayhook\n\u001B[1;32m    563\u001B[0m \u001B[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001B[39;00m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;66;03m# to pick up\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/protocol.py:471\u001B[0m, in \u001B[0;36mPy4JJavaError.__str__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__str__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    470\u001B[0m     gateway_client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_exception\u001B[38;5;241m.\u001B[39m_gateway_client\n\u001B[0;32m--> 471\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexception_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m     return_value \u001B[38;5;241m=\u001B[39m get_return_value(answer, gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001B[39;00m\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001B[39;00m\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:11:44.574421Z",
     "start_time": "2025-02-13T14:11:44.574360Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_gbt = cv_gbt.fit(df_offline) # fits pipeline_gbt with paramGridGBT",
   "id": "56f5674650897d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:11:44.575755Z",
     "start_time": "2025-02-13T14:11:44.575303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestF1_lr = max(cvModel_lr.avgMetrics)  # best F1 from logistic regression grid search\n",
    "bestF1_rf = max(cvModel_rf.avgMetrics)  # best F1 from random forest grid search\n",
    "bestF1_gbt = max(cvModel_gbt.avgMetrics) # best F1 from GBT grid search\n",
    "\n",
    "print(\"LR best F1: \", bestF1_lr)\n",
    "print(\"RF best F1: \", bestF1_rf)\n",
    "print(\"GBT best F1:\", bestF1_gbt)\n",
    "\n",
    "# Determine overall best\n",
    "if bestF1_lr >= bestF1_rf and bestF1_lr >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_lr.bestModel\n",
    "    best_score = bestF1_lr\n",
    "    best_model_name = \"LogisticRegression\"\n",
    "elif bestF1_rf >= bestF1_lr and bestF1_rf >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_rf.bestModel\n",
    "    best_score = bestF1_rf\n",
    "    best_model_name = \"RandomForest\"\n",
    "else:\n",
    "    best_overall_model = cvModel_gbt.bestModel\n",
    "    best_score = bestF1_gbt\n",
    "    best_model_name = \"GBT\"\n",
    "\n",
    "print(f\"Overall best model is {best_model_name} with F1 = {best_score}\")"
   ],
   "id": "8958f0123755436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:11:44.576404Z",
     "start_time": "2025-02-13T14:11:44.576342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_overall_model.write().overwrite().save(trained_model_path)\n",
    "print(f\"Saved best model ({best_model_name}) to {trained_model_path}\")"
   ],
   "id": "4bde307f2efa918c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "de72d920d688d8dd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
