{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T14:27:01.047691Z",
     "start_time": "2025-02-13T14:27:00.103206Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:27:01.635792Z",
     "start_time": "2025-02-13T14:27:01.632965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "data_path = \"/Users/alek/Downloads/H3/MMDS_H3/data\"\n",
    "dataset_path = os.path.join(data_path, \"dataset/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "trained_model_path = os.path.join(data_path, \"trained_models\")\n",
    "\n",
    "offline_path = os.path.join(trained_model_path, \"offline.csv\")\n",
    "online_path = os.path.join(trained_model_path, \"online.csv\")\n"
   ],
   "id": "639c0d09f9bffca3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create 80/20 random split",
   "id": "eb5f20681c03d4d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:50:58.872182Z",
     "start_time": "2025-02-13T12:50:56.441770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Diabetes_binary is the label column\n",
    "y = df[\"Diabetes_binary\"]  \n",
    "X = df.drop(\"Diabetes_binary\", axis=1)\n",
    "\n",
    "X_offline, X_online, y_offline, y_online = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X_offline + y_offline and X_online + y_online\n",
    "offline = pd.concat([X_offline, y_offline], axis=1)\n",
    "online = pd.concat([X_online, y_online], axis=1)\n",
    "\n",
    "offline.to_csv(offline_path, index=False)\n",
    "online.to_csv(online_path, index=False)"
   ],
   "id": "a62fc7717dd86f17",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Offline Spark functionality",
   "id": "de8e5c16cad24e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:27:11.301329Z",
     "start_time": "2025-02-13T14:27:06.580752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OfflineModelTraining\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_offline = spark.read.csv(offline_path, header=True, inferSchema=True) # we're inferring the schema, but we can create one as well\n",
    "\n",
    "label_column = \"Diabetes_binary\"\n",
    "feature_columns = [c for c in df_offline.columns if c != label_column]\n",
    "df_offline = df_offline.withColumnRenamed(label_column, \"label\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\") # assembles feature vector\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withStd = True, withMean = False)\n",
    "\n",
    "# Classifiers\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol=\"label\")\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol=\"label\")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "paramGridLR = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridRF = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [10, 20, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt', 'log2'])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridGBT = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10, 20])\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=pipeline_lr,\n",
    "    estimatorParamMaps=paramGridLR,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGridRF,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGridGBT,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ],
   "id": "1e38d1575db96241",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 15:27:07 WARN Utils: Your hostname, Aleks-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.180.135 instead (on interface en0)\n",
      "25/02/13 15:27:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/13 15:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:27:42.822942Z",
     "start_time": "2025-02-13T14:27:18.699083Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_lr = cv_lr.fit(df_offline)   # fits pipeline_lr with paramGridLR",
   "id": "5aa29e548470c9e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 15:27:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/02/13 15:27:19 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/02/13 15:27:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/13 15:27:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:30:22.889Z",
     "start_time": "2025-02-13T14:27:42.825986Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_rf = cv_rf.fit(df_offline)   # fits pipeline_rf with paramGridRF",
   "id": "bd5357b55e5780d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 15:27:47 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n",
      "25/02/13 15:27:49 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n",
      "25/02/13 15:27:50 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n",
      "25/02/13 15:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1456.1 KiB\n",
      "25/02/13 15:27:56 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:27:57 WARN DAGScheduler: Broadcasting large task binary with size 1343.1 KiB\n",
      "25/02/13 15:27:58 WARN DAGScheduler: Broadcasting large task binary with size 1456.1 KiB\n",
      "25/02/13 15:27:59 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:28:00 WARN DAGScheduler: Broadcasting large task binary with size 1343.1 KiB\n",
      "25/02/13 15:28:02 WARN DAGScheduler: Broadcasting large task binary with size 1456.1 KiB\n",
      "25/02/13 15:28:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 15:28:03 WARN DAGScheduler: Broadcasting large task binary with size 1343.1 KiB\n",
      "25/02/13 15:28:12 WARN DAGScheduler: Broadcasting large task binary with size 1865.0 KiB\n",
      "25/02/13 15:28:13 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 15:28:15 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:28:16 WARN DAGScheduler: Broadcasting large task binary with size 1522.1 KiB\n",
      "25/02/13 15:28:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:28:20 WARN DAGScheduler: Broadcasting large task binary with size 1865.0 KiB\n",
      "25/02/13 15:28:21 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 15:28:23 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:28:25 WARN DAGScheduler: Broadcasting large task binary with size 1522.1 KiB\n",
      "25/02/13 15:28:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:28:29 WARN DAGScheduler: Broadcasting large task binary with size 1865.0 KiB\n",
      "25/02/13 15:28:30 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 15:28:32 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:28:34 WARN DAGScheduler: Broadcasting large task binary with size 1522.1 KiB\n",
      "25/02/13 15:28:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:28:39 WARN DAGScheduler: Broadcasting large task binary with size 1273.9 KiB\n",
      "25/02/13 15:28:41 WARN DAGScheduler: Broadcasting large task binary with size 1273.9 KiB\n",
      "25/02/13 15:28:42 WARN DAGScheduler: Broadcasting large task binary with size 1273.9 KiB\n",
      "25/02/13 15:28:47 WARN DAGScheduler: Broadcasting large task binary with size 1412.2 KiB\n",
      "25/02/13 15:28:48 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:28:49 WARN DAGScheduler: Broadcasting large task binary with size 1266.2 KiB\n",
      "25/02/13 15:28:50 WARN DAGScheduler: Broadcasting large task binary with size 1412.2 KiB\n",
      "25/02/13 15:28:51 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:28:52 WARN DAGScheduler: Broadcasting large task binary with size 1266.2 KiB\n",
      "25/02/13 15:28:53 WARN DAGScheduler: Broadcasting large task binary with size 1412.2 KiB\n",
      "25/02/13 15:28:54 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:28:54 WARN DAGScheduler: Broadcasting large task binary with size 1266.2 KiB\n",
      "25/02/13 15:29:04 WARN DAGScheduler: Broadcasting large task binary with size 1852.9 KiB\n",
      "25/02/13 15:29:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:29:07 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:29:09 WARN DAGScheduler: Broadcasting large task binary with size 1493.3 KiB\n",
      "25/02/13 15:29:09 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:29:12 WARN DAGScheduler: Broadcasting large task binary with size 1852.9 KiB\n",
      "25/02/13 15:29:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:29:15 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:29:17 WARN DAGScheduler: Broadcasting large task binary with size 1493.3 KiB\n",
      "25/02/13 15:29:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:29:20 WARN DAGScheduler: Broadcasting large task binary with size 1852.9 KiB\n",
      "25/02/13 15:29:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:29:24 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/02/13 15:29:26 WARN DAGScheduler: Broadcasting large task binary with size 1493.3 KiB\n",
      "25/02/13 15:29:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:29:30 WARN DAGScheduler: Broadcasting large task binary with size 1297.7 KiB\n",
      "25/02/13 15:29:32 WARN DAGScheduler: Broadcasting large task binary with size 1297.7 KiB\n",
      "25/02/13 15:29:34 WARN DAGScheduler: Broadcasting large task binary with size 1297.7 KiB\n",
      "25/02/13 15:29:39 WARN DAGScheduler: Broadcasting large task binary with size 1421.0 KiB\n",
      "25/02/13 15:29:39 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:29:40 WARN DAGScheduler: Broadcasting large task binary with size 1270.6 KiB\n",
      "25/02/13 15:29:42 WARN DAGScheduler: Broadcasting large task binary with size 1421.0 KiB\n",
      "25/02/13 15:29:42 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:29:43 WARN DAGScheduler: Broadcasting large task binary with size 1270.6 KiB\n",
      "25/02/13 15:29:45 WARN DAGScheduler: Broadcasting large task binary with size 1421.0 KiB\n",
      "25/02/13 15:29:45 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/02/13 15:29:46 WARN DAGScheduler: Broadcasting large task binary with size 1270.6 KiB\n",
      "25/02/13 15:29:55 WARN DAGScheduler: Broadcasting large task binary with size 1858.6 KiB\n",
      "25/02/13 15:29:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:29:58 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:30:00 WARN DAGScheduler: Broadcasting large task binary with size 1513.7 KiB\n",
      "25/02/13 15:30:01 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:30:04 WARN DAGScheduler: Broadcasting large task binary with size 1858.6 KiB\n",
      "25/02/13 15:30:06 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:30:08 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:30:10 WARN DAGScheduler: Broadcasting large task binary with size 1513.7 KiB\n",
      "25/02/13 15:30:10 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:30:14 WARN DAGScheduler: Broadcasting large task binary with size 1858.6 KiB\n",
      "25/02/13 15:30:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/02/13 15:30:17 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 15:30:19 WARN DAGScheduler: Broadcasting large task binary with size 1513.7 KiB\n",
      "25/02/13 15:30:20 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 15:30:22 WARN DAGScheduler: Broadcasting large task binary with size 1343.0 KiB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:30:56.238080Z",
     "start_time": "2025-02-13T14:30:22.890086Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_gbt = cv_gbt.fit(df_offline) # fits pipeline_gbt with paramGridGBT",
   "id": "56f5674650897d0f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:30:56.245035Z",
     "start_time": "2025-02-13T14:30:56.239496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestF1_lr = max(cvModel_lr.avgMetrics)  # best F1 from logistic regression grid search\n",
    "bestF1_rf = max(cvModel_rf.avgMetrics)  # best F1 from random forest grid search\n",
    "bestF1_gbt = max(cvModel_gbt.avgMetrics) # best F1 from GBT grid search\n",
    "\n",
    "print(\"LR best F1: \", bestF1_lr)\n",
    "print(\"RF best F1: \", bestF1_rf)\n",
    "print(\"GBT best F1:\", bestF1_gbt)\n",
    "\n",
    "# Determine overall best\n",
    "if bestF1_lr >= bestF1_rf and bestF1_lr >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_lr.bestModel\n",
    "    best_score = bestF1_lr\n",
    "    best_model_name = \"LogisticRegression\"\n",
    "elif bestF1_rf >= bestF1_lr and bestF1_rf >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_rf.bestModel\n",
    "    best_score = bestF1_rf\n",
    "    best_model_name = \"RandomForest\"\n",
    "else:\n",
    "    best_overall_model = cvModel_gbt.bestModel\n",
    "    best_score = bestF1_gbt\n",
    "    best_model_name = \"GBT\"\n",
    "\n",
    "print(f\"Overall best model is {best_model_name} with F1 = {best_score}\")"
   ],
   "id": "8958f0123755436",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR best F1:  0.8269692260838889\n",
      "RF best F1:  0.8231307961690278\n",
      "GBT best F1: 0.8301072199885207\n",
      "Overall best model is GBT with F1 = 0.8301072199885207\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:30:57.408553Z",
     "start_time": "2025-02-13T14:30:56.245862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_overall_model.write().overwrite().save(trained_model_path)\n",
    "print(f\"Saved best model ({best_model_name}) to {trained_model_path}\")"
   ],
   "id": "4bde307f2efa918c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (GBT) to /Users/alek/Downloads/H3/MMDS_H3/data/trained_models\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:32:09.072432Z",
     "start_time": "2025-02-13T14:32:08.481036Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "de72d920d688d8dd",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
