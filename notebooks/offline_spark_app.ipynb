{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T17:33:02.413940Z",
     "start_time": "2025-02-13T17:33:02.406291Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:33:05.894273Z",
     "start_time": "2025-02-13T17:33:05.890255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "data_path = \"/Users/alek/Downloads/H3/MMDS_H3/data\"\n",
    "dataset_path = os.path.join(data_path, \"dataset/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "trained_model_path = os.path.join(data_path, \"trained_models\")\n",
    "\n",
    "offline_path = os.path.join(trained_model_path, \"offline.csv\")\n",
    "online_path = os.path.join(trained_model_path, \"online.csv\")\n"
   ],
   "id": "639c0d09f9bffca3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create 80/20 random split",
   "id": "eb5f20681c03d4d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:42:26.499602Z",
     "start_time": "2025-02-13T17:42:23.679555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Diabetes_binary is the label column\n",
    "y = df[\"Diabetes_binary\"]  \n",
    "X = df.drop(\"Diabetes_binary\", axis=1)\n",
    "\n",
    "X_offline, X_online, y_offline, y_online = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X_offline + y_offline and X_online + y_online\n",
    "offline = pd.concat([X_offline, y_offline], axis=1)\n",
    "online = pd.concat([X_online, y_online], axis=1)\n",
    "\n",
    "offline.to_csv(offline_path, index=False)\n",
    "online.to_csv(online_path, index=False)"
   ],
   "id": "a62fc7717dd86f17",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Offline Spark functionality",
   "id": "de8e5c16cad24e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:33:12.359344Z",
     "start_time": "2025-02-13T17:33:12.314526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OfflineModelTraining\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"HighBP\", DoubleType(), True),\n",
    "    StructField(\"HighChol\", DoubleType(), True),\n",
    "    StructField(\"CholCheck\", DoubleType(), True),\n",
    "    StructField(\"BMI\", DoubleType(), True),\n",
    "    StructField(\"Smoker\", DoubleType(), True),\n",
    "    StructField(\"Stroke\", DoubleType(), True),\n",
    "    StructField(\"HeartDiseaseorAttack\", DoubleType(), True),\n",
    "    StructField(\"PhysActivity\", DoubleType(), True),\n",
    "    StructField(\"Fruits\", DoubleType(), True),\n",
    "    StructField(\"Veggies\", DoubleType(), True),\n",
    "    StructField(\"HvyAlcoholConsump\", DoubleType(), True),\n",
    "    StructField(\"AnyHealthcare\", DoubleType(), True),\n",
    "    StructField(\"NoDocbcCost\", DoubleType(), True),\n",
    "    StructField(\"GenHlth\", DoubleType(), True),\n",
    "    StructField(\"MentHlth\", DoubleType(), True),\n",
    "    StructField(\"PhysHlth\", DoubleType(), True),\n",
    "    StructField(\"DiffWalk\", DoubleType(), True),\n",
    "    StructField(\"Sex\", DoubleType(), True),\n",
    "    StructField(\"Age\", DoubleType(), True),\n",
    "    StructField(\"Education\", DoubleType(), True),\n",
    "    StructField(\"Income\", DoubleType(), True),\n",
    "    StructField(\"Diabetes_binary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_offline = spark.read.csv(offline_path, header=True, schema=schema) # we're inferring the schema, but we can create one as well\n",
    "\n",
    "label_column = \"Diabetes_binary\"\n",
    "feature_columns = [c for c in df_offline.columns if c != label_column]\n",
    "df_offline = df_offline.withColumnRenamed(label_column, \"label\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\") # assembles feature vector\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withStd = True, withMean = False)\n",
    "\n",
    "# Classifiers\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol=\"label\")\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol=\"label\")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "paramGridLR = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridRF = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [10, 20, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt', 'log2'])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "paramGridGBT = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10, 20])\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=pipeline_lr,\n",
    "    estimatorParamMaps=paramGridLR,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGridRF,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGridGBT,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ],
   "id": "1e38d1575db96241",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:33:54.062882Z",
     "start_time": "2025-02-13T17:33:15.392507Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_lr = cv_lr.fit(df_offline)   # fits pipeline_lr with paramGridLR",
   "id": "5aa29e548470c9e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 18:33:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/13 18:33:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:39:06.110163Z",
     "start_time": "2025-02-13T17:33:54.065458Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_rf = cv_rf.fit(df_offline)   # fits pipeline_rf with paramGridRF",
   "id": "bd5357b55e5780d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 18:34:04 WARN DAGScheduler: Broadcasting large task binary with size 1326.0 KiB\n",
      "25/02/13 18:34:07 WARN DAGScheduler: Broadcasting large task binary with size 1326.0 KiB\n",
      "25/02/13 18:34:10 WARN DAGScheduler: Broadcasting large task binary with size 1326.0 KiB\n",
      "25/02/13 18:34:19 WARN DAGScheduler: Broadcasting large task binary with size 1449.1 KiB\n",
      "25/02/13 18:34:20 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:34:21 WARN DAGScheduler: Broadcasting large task binary with size 1288.2 KiB\n",
      "25/02/13 18:34:24 WARN DAGScheduler: Broadcasting large task binary with size 1449.1 KiB\n",
      "25/02/13 18:34:25 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:34:26 WARN DAGScheduler: Broadcasting large task binary with size 1288.2 KiB\n",
      "25/02/13 18:34:30 WARN DAGScheduler: Broadcasting large task binary with size 1449.1 KiB\n",
      "25/02/13 18:34:30 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:34:31 WARN DAGScheduler: Broadcasting large task binary with size 1288.2 KiB\n",
      "25/02/13 18:34:49 WARN DAGScheduler: Broadcasting large task binary with size 1870.8 KiB\n",
      "25/02/13 18:34:51 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:34:54 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:34:57 WARN DAGScheduler: Broadcasting large task binary with size 1521.1 KiB\n",
      "25/02/13 18:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:35:03 WARN DAGScheduler: Broadcasting large task binary with size 1870.8 KiB\n",
      "25/02/13 18:35:05 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:35:08 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:35:10 WARN DAGScheduler: Broadcasting large task binary with size 1521.1 KiB\n",
      "25/02/13 18:35:11 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:35:17 WARN DAGScheduler: Broadcasting large task binary with size 1870.8 KiB\n",
      "25/02/13 18:35:19 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:35:22 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:35:25 WARN DAGScheduler: Broadcasting large task binary with size 1521.1 KiB\n",
      "25/02/13 18:35:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:35:33 WARN DAGScheduler: Broadcasting large task binary with size 1318.7 KiB\n",
      "25/02/13 18:35:36 WARN DAGScheduler: Broadcasting large task binary with size 1318.7 KiB\n",
      "25/02/13 18:35:39 WARN DAGScheduler: Broadcasting large task binary with size 1318.7 KiB\n",
      "25/02/13 18:35:50 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "25/02/13 18:35:51 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:35:52 WARN DAGScheduler: Broadcasting large task binary with size 1296.7 KiB\n",
      "25/02/13 18:35:56 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "25/02/13 18:35:57 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:35:58 WARN DAGScheduler: Broadcasting large task binary with size 1296.7 KiB\n",
      "25/02/13 18:36:01 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "25/02/13 18:36:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:36:04 WARN DAGScheduler: Broadcasting large task binary with size 1296.7 KiB\n",
      "25/02/13 18:36:22 WARN DAGScheduler: Broadcasting large task binary with size 1866.1 KiB\n",
      "25/02/13 18:36:25 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:36:27 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:36:30 WARN DAGScheduler: Broadcasting large task binary with size 1536.6 KiB\n",
      "25/02/13 18:36:31 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:36:38 WARN DAGScheduler: Broadcasting large task binary with size 1866.1 KiB\n",
      "25/02/13 18:36:41 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:36:44 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:36:47 WARN DAGScheduler: Broadcasting large task binary with size 1536.6 KiB\n",
      "25/02/13 18:36:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1866.1 KiB\n",
      "25/02/13 18:36:56 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:37:00 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:37:04 WARN DAGScheduler: Broadcasting large task binary with size 1536.6 KiB\n",
      "25/02/13 18:37:05 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:37:14 WARN DAGScheduler: Broadcasting large task binary with size 1344.3 KiB\n",
      "25/02/13 18:37:17 WARN DAGScheduler: Broadcasting large task binary with size 1344.3 KiB\n",
      "25/02/13 18:37:20 WARN DAGScheduler: Broadcasting large task binary with size 1344.3 KiB\n",
      "25/02/13 18:37:30 WARN DAGScheduler: Broadcasting large task binary with size 1452.9 KiB\n",
      "25/02/13 18:37:31 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:37:33 WARN DAGScheduler: Broadcasting large task binary with size 1300.6 KiB\n",
      "25/02/13 18:37:36 WARN DAGScheduler: Broadcasting large task binary with size 1452.9 KiB\n",
      "25/02/13 18:37:37 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:37:38 WARN DAGScheduler: Broadcasting large task binary with size 1300.6 KiB\n",
      "25/02/13 18:37:42 WARN DAGScheduler: Broadcasting large task binary with size 1452.9 KiB\n",
      "25/02/13 18:37:43 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/02/13 18:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1300.6 KiB\n",
      "25/02/13 18:38:03 WARN DAGScheduler: Broadcasting large task binary with size 1866.5 KiB\n",
      "25/02/13 18:38:05 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:38:08 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:38:10 WARN DAGScheduler: Broadcasting large task binary with size 1526.6 KiB\n",
      "25/02/13 18:38:11 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:38:18 WARN DAGScheduler: Broadcasting large task binary with size 1866.5 KiB\n",
      "25/02/13 18:38:20 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:38:22 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:38:25 WARN DAGScheduler: Broadcasting large task binary with size 1526.6 KiB\n",
      "25/02/13 18:38:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:38:33 WARN DAGScheduler: Broadcasting large task binary with size 1866.5 KiB\n",
      "25/02/13 18:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:38:39 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/02/13 18:38:42 WARN DAGScheduler: Broadcasting large task binary with size 1526.6 KiB\n",
      "25/02/13 18:38:43 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/02/13 18:38:54 WARN DAGScheduler: Broadcasting large task binary with size 1869.0 KiB\n",
      "25/02/13 18:38:57 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/02/13 18:39:00 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/02/13 18:39:05 WARN DAGScheduler: Broadcasting large task binary with size 1642.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:40:02.217702Z",
     "start_time": "2025-02-13T17:39:06.111777Z"
    }
   },
   "cell_type": "code",
   "source": "cvModel_gbt = cv_gbt.fit(df_offline) # fits pipeline_gbt with paramGridGBT",
   "id": "56f5674650897d0f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:40:02.228575Z",
     "start_time": "2025-02-13T17:40:02.221588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestF1_lr = max(cvModel_lr.avgMetrics)  # best F1 from logistic regression grid search\n",
    "bestF1_rf = max(cvModel_rf.avgMetrics)  # best F1 from random forest grid search\n",
    "bestF1_gbt = max(cvModel_gbt.avgMetrics) # best F1 from GBT grid search\n",
    "\n",
    "print(\"LR best F1: \", bestF1_lr)\n",
    "print(\"RF best F1: \", bestF1_rf)\n",
    "print(\"GBT best F1:\", bestF1_gbt)\n",
    "\n",
    "# Determine overall best\n",
    "if bestF1_lr >= bestF1_rf and bestF1_lr >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_lr.bestModel\n",
    "    best_score = bestF1_lr\n",
    "    best_model_name = \"LogisticRegression\"\n",
    "elif bestF1_rf >= bestF1_lr and bestF1_rf >= bestF1_gbt:\n",
    "    best_overall_model = cvModel_rf.bestModel\n",
    "    best_score = bestF1_rf\n",
    "    best_model_name = \"RandomForest\"\n",
    "else:\n",
    "    best_overall_model = cvModel_gbt.bestModel\n",
    "    best_score = bestF1_gbt\n",
    "    best_model_name = \"GBT\"\n",
    "\n",
    "print(f\"Overall best model is {best_model_name} with F1 = {best_score}\")"
   ],
   "id": "8958f0123755436",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR best F1:  0.826976776194452\n",
      "RF best F1:  0.8222871575772458\n",
      "GBT best F1: 0.8296026843595836\n",
      "Overall best model is GBT with F1 = 0.8296026843595836\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:40:04.252910Z",
     "start_time": "2025-02-13T17:40:02.229737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_overall_model.write().overwrite().save(trained_model_path)\n",
    "print(f\"Saved best model ({best_model_name}) to {trained_model_path}\")"
   ],
   "id": "4bde307f2efa918c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (GBT) to /Users/alek/Downloads/H3/MMDS_H3/data/trained_models\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:40:04.325090Z",
     "start_time": "2025-02-13T17:40:04.254205Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "de72d920d688d8dd",
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
