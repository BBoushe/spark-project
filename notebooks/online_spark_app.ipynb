{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T17:49:08.966588Z",
     "start_time": "2025-02-13T17:49:08.963556Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os.path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.ml import PipelineModel\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:43:49.537828Z",
     "start_time": "2025-02-13T17:43:49.533908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "data_path = \"/Users/alek/Downloads/H3/MMDS_H3/data\"\n",
    "dataset_path = os.path.join(data_path, \"dataset/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "trained_model_path = os.path.join(data_path, \"trained_models\")\n",
    "\n",
    "offline_path = os.path.join(trained_model_path, \"offline.csv\")\n",
    "online_path = os.path.join(trained_model_path, \"online.csv\")"
   ],
   "id": "2fa27639fc70f93e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4df2740db423f594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:51:22.670192Z",
     "start_time": "2025-02-13T17:49:11.372981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineSparkApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_kafka = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "    .option('subscribe', 'health_indicators') \\\n",
    "    .load()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"HighBP\", DoubleType(), True),\n",
    "    StructField(\"HighChol\", DoubleType(), True),\n",
    "    StructField(\"CholCheck\", DoubleType(), True),\n",
    "    StructField(\"BMI\", DoubleType(), True),\n",
    "    StructField(\"Smoker\", DoubleType(), True),\n",
    "    StructField(\"Stroke\", DoubleType(), True),\n",
    "    StructField(\"HeartDiseaseorAttack\", DoubleType(), True),\n",
    "    StructField(\"PhysActivity\", DoubleType(), True),\n",
    "    StructField(\"Fruits\", DoubleType(), True),\n",
    "    StructField(\"Veggies\", DoubleType(), True),\n",
    "    StructField(\"HvyAlcoholConsump\", DoubleType(), True),\n",
    "    StructField(\"AnyHealthcare\", DoubleType(), True),\n",
    "    StructField(\"NoDocbcCost\", DoubleType(), True),\n",
    "    StructField(\"GenHlth\", DoubleType(), True),\n",
    "    StructField(\"MentHlth\", DoubleType(), True),\n",
    "    StructField(\"PhysHlth\", DoubleType(), True),\n",
    "    StructField(\"DiffWalk\", DoubleType(), True),\n",
    "    StructField(\"Sex\", DoubleType(), True),\n",
    "    StructField(\"Age\", DoubleType(), True),\n",
    "    StructField(\"Education\", DoubleType(), True),\n",
    "    StructField(\"Income\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_parsed = df_kafka.selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "df_parsed = df_parsed.na.fill({\n",
    "    \"HighBP\": 0,\n",
    "    \"HighChol\": 0,\n",
    "    \"CholCheck\": 0,\n",
    "    \"BMI\": 0.0,\n",
    "    \"Smoker\": 0,\n",
    "    \"Stroke\": 0,\n",
    "    \"HeartDiseaseorAttack\": 0,\n",
    "    \"PhysActivity\": 0,\n",
    "    \"Fruits\": 0,\n",
    "    \"Veggies\": 0,\n",
    "    \"HvyAlcoholConsump\": 0,\n",
    "    \"AnyHealthcare\": 0,\n",
    "    \"NoDocbcCost\": 0,\n",
    "    \"GenHlth\": 0,\n",
    "    \"MentHlth\": 0,\n",
    "    \"PhysHlth\": 0,\n",
    "    \"DiffWalk\": 0,\n",
    "    \"Sex\": 0,\n",
    "    \"Age\": 0,\n",
    "    \"Education\": 0,\n",
    "    \"Income\": 0\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "bestModel = PipelineModel.load(trained_model_path)\n",
    "\n",
    "df_predicted = bestModel.transform(df_parsed)\n",
    "\n",
    "df_output = df_predicted.select(\n",
    "    to_json(struct(\"*\")).alias(\"value\")\n",
    ")\n",
    "\n",
    "# 6) Send enriched data to new topic\n",
    "query = df_output \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"health_indicators_predicted\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoint_health_indicators_predicted\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "id": "5f377210a24fa6fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 18:49:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/02/13 18:49:12 WARN StreamingQueryManager: Stopping existing streaming query [id=bf01a924-9a6c-4488-90cd-eee52fe0f3fd, runId=b24f00a3-f06c-4915-a7e1-061f39c18da0], as a new run is being started.\n",
      "25/02/13 18:49:12 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1259, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24127779] is aborting.\n",
      "25/02/13 18:49:12 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1259, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24127779] aborted.\n",
      "25/02/13 18:49:12 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "25/02/13 18:49:12 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 230, attempt 0, stage 230.0)\n",
      "25/02/13 18:49:12 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 230, attempt 0, stage 230.0)\n",
      "25/02/13 18:49:12 WARN TaskSetManager: Lost task 0.0 in stage 230.0 (TID 230) (192.168.180.135 executor driver): TaskKilled (Stage cancelled: Job 227 cancelled part of cancelled job group b24f00a3-f06c-4915-a7e1-061f39c18da0)\n",
      "25/02/13 18:49:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/alek/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 86\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# 6) Send enriched data to new topic\u001B[39;00m\n\u001B[1;32m     78\u001B[0m query \u001B[38;5;241m=\u001B[39m df_output \\\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkafka\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/spark_checkpoint_health_indicators_predicted\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m---> 86\u001B[0m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m~/Downloads/H3/MMDS_H3/venv/lib/python3.9/site-packages/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
